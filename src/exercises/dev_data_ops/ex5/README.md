Task 5: Building a Scalable Data Pipeline
Objective: Demonstrate your ability to create a scalable data pipeline using Python and Airflow.

Problem Statement:
You are tasked with building a data pipeline that ingests data from an online source, processes it, and stores the processed data in a database. The pipeline should be scheduled to run daily.

Requirements:

Use Apache Airflow to schedule and manage the pipeline.
Fetch data from a public API (e.g., a weather API).
Process the data (e.g., clean, transform, aggregate).
Store the processed data in a SQLite database.
Deliverables:

Airflow DAG script.
Python script for data ingestion and processing.
SQLite database file with the processed data.
Instructions to set up and run the Airflow pipeline.